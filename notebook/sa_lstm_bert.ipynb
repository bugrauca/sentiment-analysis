{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vZjte3nw63Wm"},"outputs":[],"source":["!pip install transformers datasets scikit-learn"]},{"cell_type":"markdown","source":["# Bert Hypermeter Tuning\n","` not part of the final version of the project`\n","\n"],"metadata":{"id":"ykNO-NrqDsN6"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"XJVvTShk_E6g"},"outputs":[],"source":["########## Hyperparameter Search Bert Model ##########\n","########## NOT used in the final version of the project ##########\n","\n","import torch\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm import tqdm\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from transformers import get_scheduler\n","from sklearn.metrics import accuracy_score, classification_report\n","import torch.nn as nn\n","import itertools\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n","print(f\"Using device: {DEVICE}\")\n","\n","\n","# Dataset Class\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=128):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        encoding = self.tokenizer(\n","            self.texts[idx],\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=self.max_length,\n","            return_tensors=\"pt\"\n","        )\n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n","            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n","            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n","        }\n","\n","# Load Preprocessed Data\n","processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","print(\"Loading preprocessed data...\")\n","data = pd.read_csv(processed_data_path)\n","\n","# Verify dataset class distribution (add after loading your dataset)\n","print(\"Class distribution in the dataset:\")\n","print(data[\"sentiment\"].value_counts())\n","\n","# Limit dataset size for testing\n","subset_size = 100000  # Use a subset for quick iterations\n","data = data.sample(subset_size, random_state=42)\n","\n","# Drop rows with missing values\n","data.dropna(subset=[\"text\"], inplace=True)\n","\n","# Prepare tokenizer\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","# Prepare datasets and dataloaders\n","train_texts, test_texts, train_labels, test_labels = train_test_split(\n","    data[\"text\"], data[\"sentiment\"], test_size=0.2, random_state=42\n",")\n","\n","train_dataset = SentimentDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n","test_dataset = SentimentDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","# Define DataLoaders for training and validation\n","val_texts, val_labels = test_texts, test_labels\n","val_dataset = SentimentDataset(val_texts.tolist(), val_labels.tolist(), tokenizer)\n","val_loader = DataLoader(val_dataset, batch_size=32)\n","\n","# Initialize BERT model\n","print(\"Initializing BERT model...\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3).to(DEVICE)\n","\n","# Optimizer and Scheduler\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","num_training_steps = len(train_loader) * 10  # Set max epochs to 10\n","scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","# Loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# Training function\n","def train_model(model, dataloader, optimizer, scheduler, criterion):\n","    model.train()\n","    total_loss = 0\n","    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n","    for batch in progress_bar:\n","        input_ids = batch[\"input_ids\"].to(DEVICE)\n","        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","        labels = batch[\"labels\"].to(DEVICE)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        total_loss += loss.item()\n","        progress_bar.set_postfix(loss=loss.item())\n","    return total_loss / len(dataloader)\n","\n","# Evaluation function\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","    predictions_list = []\n","    true_labels_list = []\n","\n","    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n","    with torch.no_grad():\n","        for batch in progress_bar:\n","            input_ids = batch[\"input_ids\"].to(DEVICE)\n","            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","            labels = batch[\"labels\"].to(DEVICE)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","\n","            predictions = torch.argmax(outputs.logits, dim=-1)\n","            predictions_list.extend(predictions.cpu().numpy())\n","            true_labels_list.extend(labels.cpu().numpy())\n","\n","            correct += (predictions == labels).sum().item()\n","            total += labels.size(0)\n","            progress_bar.set_postfix(loss=loss.item(), accuracy=correct / total)\n","\n","    accuracy = correct / total\n","    return total_loss / len(dataloader), accuracy, predictions_list, true_labels_list\n","\n","# Define hyperparameter search space\n","search_space = {\n","    \"learning_rate\": [2e-5, 3e-5, 5e-5],\n","    \"batch_size\": [16, 32],\n","    \"num_epochs\": [2, 3]\n","}\n","\n","def hyperparameter_tuning(search_space, train_loader, val_loader):\n","    \"\"\"\n","    Perform hyperparameter tuning to find the best configuration.\n","    Args:\n","        search_space (dict): Dictionary containing hyperparameter lists.\n","        train_loader (DataLoader): DataLoader for training data.\n","        val_loader (DataLoader): DataLoader for validation data.\n","\n","    Returns:\n","        dict: Best hyperparameters and associated accuracy.\n","    \"\"\"\n","    best_config = None\n","    best_accuracy = 0.0\n","\n","    # Generate all combinations of hyperparameters\n","    combinations = list(itertools.product(\n","        search_space[\"learning_rate\"],\n","        search_space[\"batch_size\"],\n","        search_space[\"num_epochs\"]\n","    ))\n","\n","    for lr, batch_size, num_epochs in tqdm(combinations, desc=\"Tuning Hyperparameters\"):\n","        print(f\"\\nTesting configuration: LR={lr}, Batch Size={batch_size}, Epochs={num_epochs}\")\n","\n","        # Update DataLoader with current batch size\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n","\n","        # Initialize model, optimizer, and scheduler\n","        model = BertForSequenceClassification.from_pretrained(\n","            \"bert-base-uncased\",\n","            num_labels=3\n","        ).to(DEVICE)\n","        optimizer = AdamW(model.parameters(), lr=lr)\n","        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n","        criterion = nn.CrossEntropyLoss()\n","\n","        # Train the model\n","        for epoch in range(num_epochs):\n","            train_loss = train_model(model, train_loader, optimizer, scheduler, criterion)\n","            print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}\")\n","\n","        # Validate the model\n","        _, val_accuracy, _, _ = evaluate_model(model, val_loader, criterion)\n","        print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n","\n","        # Update the best configuration if the current one is better\n","        if val_accuracy > best_accuracy:\n","            best_accuracy = val_accuracy\n","            best_config = {\n","                \"learning_rate\": lr,\n","                \"batch_size\": batch_size,\n","                \"num_epochs\": num_epochs\n","            }\n","            # Save the best model\n","            torch.save(model.state_dict(), \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/tryouts/bert_model_best.pt\")\n","\n","    print(f\"\\nBest Configuration: {best_config}\")\n","    print(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\n","    return best_config\n","\n","print(\"Starting hyperparameter tuning...\")\n","best_hyperparameters = hyperparameter_tuning(search_space, train_loader, val_loader)\n","print(f\"Best Hyperparameters: {best_hyperparameters}\")\n","\n","# Evaluate Model\n","print(\"Evaluating model...\")\n","model.load_state_dict(torch.load(\"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/tryouts/bert_model_best.pt\"))\n","model.eval()\n","\n","# Get predictions and true labels\n","val_loss, val_accuracy, predictions, true_labels = evaluate_model(model, test_loader, criterion)\n","\n","# Print Accuracy Score and Classification Report\n","accuracy = accuracy_score(true_labels, predictions)\n","print(f\"Accuracy: {accuracy:.4f}\")\n","\n","# Adjust dynamically based on unique labels\n","unique_labels = sorted(list(set(true_labels + predictions)))\n","\n","# Define the target names dynamically\n","target_names_full = [\"Negative\", \"Neutral\", \"Positive\"]\n","target_names = [target_names_full[i] for i in unique_labels]\n","\n","# Print the classification report\n","print(\"\\nClassification Report:\")\n","print(classification_report(true_labels, predictions, target_names=target_names))"]},{"cell_type":"markdown","source":["# Final BERT Model and Evaluation\n","` used in the final version of the project`"],"metadata":{"id":"RgKAjY76dJZI"}},{"cell_type":"code","source":["########## Bert Model Training Script ##########\n","\n","import os\n","import torch\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import DataLoader, Dataset\n","from tqdm import tqdm\n","from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n","from transformers import get_scheduler\n","from sklearn.metrics import accuracy_score, classification_report\n","import torch.nn as nn\n","from torch.cuda.amp import autocast, GradScaler\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {DEVICE}\")\n","\n","# Dataset Class\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=128):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        encoding = self.tokenizer(\n","            self.texts[idx],\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=self.max_length,\n","            return_tensors=\"pt\"\n","        )\n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n","            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n","            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n","        }\n","\n","model_save_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/bert_model.pt\"\n","output_save_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/results/metrics\"\n","os.makedirs(output_save_path, exist_ok=True)\n","\n","# Load Preprocessed Data\n","processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","print(\"Loading preprocessed data...\")\n","data = pd.read_csv(processed_data_path)\n","\n","# Verify dataset class distribution\n","print(\"Class distribution in the dataset:\")\n","print(data[\"sentiment\"].value_counts())\n","\n","# Limit dataset size for testing\n","subset_size = 100000\n","data = data.sample(subset_size, random_state=42)\n","\n","# Drop rows with missing values\n","data.dropna(subset=[\"text\"], inplace=True)\n","\n","# Prepare tokenizer\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","# Prepare datasets and dataloaders\n","train_texts, test_texts, train_labels, test_labels = train_test_split(\n","    data[\"text\"], data[\"sentiment\"], test_size=0.2, random_state=42\n",")\n","\n","train_dataset = SentimentDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n","test_dataset = SentimentDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n","\n","train_loader = DataLoader(\n","    train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True\n",")\n","test_loader = DataLoader(\n","    test_dataset, batch_size=16, num_workers=4, pin_memory=True\n",")\n","\n","print(\"Initializing BERT model...\")\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3).to(DEVICE)\n","\n","# Optimizer and Scheduler\n","optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n","num_training_steps = len(train_loader) * 3  # Adjust for 3 epochs\n","warmup_steps = int(0.1 * num_training_steps)\n","scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps)\n","\n","# Loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","# Mixed Precision Training\n","scaler = GradScaler()\n","\n","# Gradient Accumulation Steps\n","accumulation_steps = 2  # Simulates batch size 32\n","\n","def train_model(model, dataloader, optimizer, scheduler, criterion):\n","    model.train()\n","    total_loss = 0\n","    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n","    optimizer.zero_grad()\n","\n","    for i, batch in enumerate(progress_bar):\n","        input_ids = batch[\"input_ids\"].to(DEVICE)\n","        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","        labels = batch[\"labels\"].to(DEVICE)\n","\n","        with autocast():\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss / accumulation_steps  # Divide loss for accumulation\n","\n","        scaler.scale(loss).backward()\n","\n","        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):\n","            scaler.unscale_(optimizer)\n","            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            scheduler.step()\n","\n","        total_loss += loss.item() * accumulation_steps  # Scale back for correct total\n","        progress_bar.set_postfix(loss=loss.item() * accumulation_steps)\n","    return total_loss / len(dataloader)\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    correct = 0\n","    total = 0\n","    predictions_list = []\n","    true_labels_list = []\n","\n","    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n","    with torch.no_grad():\n","        for batch in progress_bar:\n","            input_ids = batch[\"input_ids\"].to(DEVICE)\n","            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","            labels = batch[\"labels\"].to(DEVICE)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","\n","            predictions = torch.argmax(outputs.logits, dim=-1)\n","            predictions_list.extend(predictions.cpu().numpy())\n","            true_labels_list.extend(labels.cpu().numpy())\n","\n","            correct += (predictions == labels).sum().item()\n","            total += labels.size(0)\n","            progress_bar.set_postfix(loss=loss.item(), accuracy=correct / total)\n","\n","    accuracy = correct / total\n","    return total_loss / len(dataloader), accuracy, predictions_list, true_labels_list\n","\n","# Training loop with checkpoint saving\n","train_losses = []\n","val_losses = []\n","val_accuracies = []\n","best_accuracy = 0\n","\n","print(\"Training model...\")\n","for epoch in range(3):  # Train for 3 epochs\n","    print(f\"Epoch {epoch + 1}\")\n","    train_loss = train_model(model, train_loader, optimizer, scheduler, criterion)\n","    val_loss, val_accuracy, predictions, true_labels = evaluate_model(model, test_loader, criterion)\n","\n","    train_losses.append(train_loss)\n","    val_losses.append(val_loss)\n","    val_accuracies.append(val_accuracy)\n","\n","    print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n","\n","    if val_accuracy > best_accuracy:\n","        best_accuracy = val_accuracy\n","        torch.save(model.state_dict(), model_save_path)\n","        print(f\"Model saved at epoch {epoch + 1}\")\n","\n","print(\"Evaluating model...\")\n","model.load_state_dict(torch.load(model_save_path))\n","model.eval()\n","\n","val_loss, val_accuracy, predictions, true_labels = evaluate_model(model, test_loader, criterion)\n","\n","print(\"Saving outputs for visualization...\")\n","outputs = {\n","    \"train_losses\": train_losses,\n","    \"val_losses\": val_losses,\n","    \"val_accuracies\": val_accuracies,\n","    \"predictions\": predictions,\n","    \"true_labels\": true_labels\n","}\n","torch.save(outputs, os.path.join(output_save_path, \"bert_visualization_outputs.pth\"))\n","\n","accuracy = accuracy_score(true_labels, predictions)\n","print(f\"Final Accuracy: {accuracy:.4f}\")\n","\n","target_names = [\"Negative\", \"Neutral\", \"Positive\"]\n","print(\"\\nClassification Report:\")\n","print(classification_report(true_labels, predictions, target_names=target_names))\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["96b72f6e7fe34912a8936e5fdb3c08ca","4017635b677f40d3a66330c0e7460603","79add1aa667441fd9441671335003411","b71e8c590c5045e0859c2e5b498138d9","27a1723962df444db8c9c020ea30e49e","69e326f98da44bec84d049c2c61bd0d0","f867c03820cd4f12995a822b739275a9","21f625ec2e1644d4ad6f351acd0ffd28","a098da9765704755b8bf22c859f84829","c0d25bb222bf4b5498ebccb1aacd2598","86b4b1ca87264494b2327cf2c639c80f","c3abfe77876a4ce0a65b1221f72f413a","94917daf0b8c4b89b1627ab755c72472","5827887db6464a609ab61b2b9e3bd383","45f71293c66e47479e9ee589ea247450","003be98a6bb2480a8ed54ee59a46832b","cf32d0766a7b4deeaffbfb3c1f572d2b","51888b94585a46b8873aa684710a85ba","34de4187eb7f49f58fdb3de21d1261a2","312cd83d7aaa409dbf9d117a0f59f640","e5359ba9e6634305bf32d564b9b16fc5","d33a3c8cba894bafa61f41fb2d45bbe3","09dd094ecdc2418193bcd4fd4d5b5c26","3da5d9c42cd049b69e48e29f9194a0d9","14cd7cb4ad254b5492903648353c2940","83bd0e9a829140cfa9b1dbdfab2b2965","9c4fb0f182034c9da97cdb4b3cee15dc","b335529231b443cb8f662b8ca11197d6","0d156bb74e4841d1825acff2fc298553","de721d7624aa45de9db9dc8472fc4ee8","ddaf99001c3b4d2c8b0e0c6a2293eb36","d0a7db42d6f74a9fb40557317d951716","b9e1c557a2d343a8a05ec7b361872721","fc7f443540534bebb46342243e685dd6","68cf654b4c3f42578ebd6a28f403b87c","7f5a66af9d2a46668eee21b22e280758","9b3639fb05a64fa79315bcc77f5a2cd8","fed8747baa464bb1a73a5e77048889fa","0a97e13f873e4abf9e56147bc2298914","be30bfcfff314c42aee3fd613a7e6f2a","240f089a45404b939264b670e88c2b99","d00ffd748f994258a79628dd2ece8d1b","6a6df3ece7a44ebf9bd1e83328909e18","ea783fb21f46478ea508a9088a26fabf","3e8af17130d544a38a65a16094a557a7","a0b326ee2d3744298683a660cc3dd6b7","6787782c7ef349859f08f3d527cbe1fd","89b745e2c3d340c391a85174ce6e9180","bd6b2443cbde43a186b68c020db89720","34f7d207a3dd4048976a9cc91f50a007","92f10d814f4e440a9c926f302e339fbf","7994faa703814f2ba8413e6619a97ce0","79adbb7b670d4c0e9e58021608d7cf9f","e9cd3d7d3b33465c9456ba9dd408ab65","2a183c6550de477d9b4b65151d65a2f9"]},"id":"RCeZz96yarZH","executionInfo":{"status":"error","timestamp":1737826968510,"user_tz":-60,"elapsed":2200444,"user":{"displayName":"Buğra","userId":"13429055383207080256"}},"outputId":"2d00b773-aaf7-4178-c4dc-6cd7a1dc1ef2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Loading preprocessed data...\n","Class distribution in the dataset:\n","sentiment\n","0    800000\n","2    800000\n","Name: count, dtype: int64\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b72f6e7fe34912a8936e5fdb3c08ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3abfe77876a4ce0a65b1221f72f413a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09dd094ecdc2418193bcd4fd4d5b5c26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc7f443540534bebb46342243e685dd6"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Initializing BERT model...\n"]},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e8af17130d544a38a65a16094a557a7"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","<ipython-input-2-e10d40603bf3>:92: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = GradScaler()\n"]},{"output_type":"stream","name":"stdout","text":["Training model...\n","Epoch 1\n"]},{"output_type":"stream","name":"stderr","text":["\rTraining:   0%|          | 0/4991 [00:00<?, ?it/s]<ipython-input-2-e10d40603bf3>:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with autocast():\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss = 0.4756, Val Loss = 0.3880, Val Accuracy = 0.8265\n","Model saved at epoch 1\n","Epoch 2\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Train Loss = 0.3210, Val Loss = 0.4050, Val Accuracy = 0.8319\n","Model saved at epoch 2\n","Epoch 3\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-e10d40603bf3>:180: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_save_path))\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Train Loss = 0.1968, Val Loss = 0.4544, Val Accuracy = 0.8305\n","Evaluating model...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Saving outputs for visualization...\n","Final Accuracy: 0.8319\n","\n","Classification Report:\n"]},{"output_type":"error","ename":"ValueError","evalue":"Number of classes, 2, does not match size of target_names, 3. Try specifying the labels parameter","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-e10d40603bf3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Negative\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Neutral\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Positive\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nClassification Report:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2691\u001b[0m             )\n\u001b[1;32m   2692\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2693\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m   2694\u001b[0m                 \u001b[0;34m\"Number of classes, {0}, does not match size of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m                 \u001b[0;34m\"target_names, {1}. Try specifying the labels \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Number of classes, 2, does not match size of target_names, 3. Try specifying the labels parameter"]}]},{"cell_type":"code","source":["########## Evaluation Bert Model ##########\n","\n","import os\n","import torch\n","import pandas as pd\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","from transformers import BertTokenizer, BertForSequenceClassification\n","import torch.nn as nn\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Set random seed for reproducibility\n","seed = 42\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","if torch.backends.cudnn.is_available():\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=128):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        encoding = self.tokenizer(\n","            self.texts[idx],\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=self.max_length,\n","            return_tensors=\"pt\"\n","        )\n","        return {\n","            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n","            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n","            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long)\n","        }\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    true_labels, predictions = [], []\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            input_ids = batch[\"input_ids\"].to(DEVICE)\n","            attention_mask = batch[\"attention_mask\"].to(DEVICE)\n","            labels = batch[\"labels\"].to(DEVICE)\n","\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs.loss\n","            total_loss += loss.item()\n","\n","            preds = torch.argmax(outputs.logits, dim=1).cpu().numpy()\n","            predictions.extend(preds)\n","            true_labels.extend(labels.cpu().numpy())\n","\n","    accuracy = accuracy_score(true_labels, predictions)\n","    return total_loss / len(dataloader), accuracy, true_labels, predictions\n","\n","if __name__ == \"__main__\":\n","    model_save_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/bert_model.pt\"\n","    processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","\n","    # Load preprocessed data\n","    print(\"Loading preprocessed data...\")\n","    data = pd.read_csv(processed_data_path)\n","    data.dropna(subset=[\"text\"], inplace=True)\n","\n","    # Prepare tokenizer and datasets\n","    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","    test_texts, test_labels = data[\"text\"].tolist(), data[\"sentiment\"].tolist()\n","    test_dataset = SentimentDataset(test_texts, test_labels, tokenizer)\n","    test_loader = DataLoader(test_dataset, batch_size=16)\n","\n","    # Load the pre-trained model\n","    print(\"Loading model...\")\n","    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=3).to(DEVICE)\n","    model.load_state_dict(torch.load(model_save_path))\n","\n","    # Evaluate the model\n","    print(\"Evaluating model...\")\n","    criterion = nn.CrossEntropyLoss()\n","    _, test_accuracy, true_labels, predictions = evaluate_model(model, test_loader, criterion)\n","\n","    unique_classes = sorted(set(true_labels))\n","    num_classes = len(unique_classes)\n","\n","    # Generate target names based on the number of classes\n","    default_target_names = [\"Class \" + str(i) for i in range(num_classes)]\n","    custom_target_names = [\"Negative\", \"Neutral\", \"Positive\"][:num_classes]  # Adjust to match detected classes\n","\n","    print(\"\\nAccuracy:\", accuracy_score(true_labels, predictions))\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(true_labels, predictions, target_names=custom_target_names))\n","\n","    print(\"\\nConfusion Matrix:\")\n","    print(confusion_matrix(true_labels, predictions, labels=unique_classes))\n"],"metadata":{"id":"dtcJeyqdly9h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# LSTM Hypermeter Tuning\n","##### `hyperparameters tuning was applied to find the best hyperparameters to get the best accuracy, it is not used in the final version of the project`"],"metadata":{"id":"-eViRJZE9R0E"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":829278,"status":"ok","timestamp":1736517938602,"user":{"displayName":"Buğra","userId":"13429055383207080256"},"user_tz":-60},"id":"wtLe4rnyd5mg","outputId":"75934323-57cc-408e-a58d-3e56ce6e623d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","Loading vectorizer...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Loading preprocessed data...\n","Splitting data into train and test sets...\n","Initializing model...\n","Using device: cuda\n","Training model...\n","Epoch 1: Loss = 0.4267\n","Epoch 2: Loss = 0.3901\n","Epoch 3: Loss = 0.3764\n","Epoch 4: Loss = 0.3658\n","Epoch 5: Loss = 0.3565\n","Saving model...\n","Model saved to /content/drive/MyDrive/Colab Notebooks/SentimentAnalysis/wout_metadata/lstm_model.pt\n","Evaluating model...\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-1-5482bf55ebc7>:137: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_save_path))\n"]},{"name":"stdout","output_type":"stream","text":["\n","Accuracy: 0.8194986316639843\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.82      0.82      0.82    159215\n","           2       0.82      0.82      0.82    160151\n","\n","    accuracy                           0.82    319366\n","   macro avg       0.82      0.82      0.82    319366\n","weighted avg       0.82      0.82      0.82    319366\n","\n","\n","Confusion Matrix:\n","[[130013  29202]\n"," [ 28444 131707]]\n"]}],"source":["import os\n","import pickle\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import pandas as pd\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n","print(f\"Using device: {DEVICE}\")\n","\n","seed = 42\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","if torch.backends.cudnn.is_available():\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=100):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokenizer(self.texts[idx])\n","        tokens = tokens[:self.max_length]  # Truncate if too long\n","        tokens = [0] * (self.max_length - len(tokens)) + tokens  # Pad if too short\n","\n","        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n","        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return tokens_tensor, label_tensor\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=1):\n","        super(LSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        hidden_state = lstm_out[:, -1, :]\n","        output = self.fc(hidden_state)\n","        return output\n","\n","def train_model(model, dataloader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0.0\n","    for inputs, labels in dataloader:\n","        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","def evaluate_model(model, dataloader):\n","    model.eval()\n","    true_labels, predictions = [], []\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(inputs)\n","            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","            predictions.extend(preds)\n","            true_labels.extend(labels.cpu().numpy())\n","    return true_labels, predictions\n","\n","if __name__ == \"__main__\":\n","\n","    vectorizer_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/vectorizer.pkl\"\n","    processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","    model_save_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/tryouts/lstm_model_base.pt\"\n","\n","    print(\"Loading vectorizer...\")\n","    with open(vectorizer_path, \"rb\") as f:\n","        vectorizer = pickle.load(f)\n","\n","    print(\"Loading preprocessed data...\")\n","    data = pd.read_csv(processed_data_path)\n","    data.dropna(subset=[\"text\"], inplace=True)\n","\n","    tokenizer_fn = vectorizer.build_analyzer()\n","\n","    def tokenizer(text):\n","        tokens = tokenizer_fn(text)\n","        return [vectorizer.vocabulary_.get(token, 0) for token in tokens]\n","\n","    print(\"Splitting data into train and test sets...\")\n","    train_texts, test_texts, train_labels, test_labels = train_test_split(\n","        data[\"text\"], data[\"sentiment\"], test_size=0.2, random_state=42\n","    )\n","\n","    train_dataset = SentimentDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n","    test_dataset = SentimentDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=64)\n","\n","    vocab_size = len(vectorizer.vocabulary_)\n","    embed_dim = 100\n","    hidden_dim = 256\n","    output_dim = 3\n","\n","    print(\"Initializing model...\")\n","    model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim).to(DEVICE)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    print(f\"Using device: {DEVICE}\")\n","\n","    print(\"Training model...\")\n","    for epoch in range(5):\n","        train_loss = train_model(model, train_loader, optimizer, criterion)\n","        print(f\"Epoch {epoch + 1}: Loss = {train_loss:.4f}\")\n","\n","    print(\"Saving model...\")\n","    os.makedirs(\"models\", exist_ok=True)\n","    torch.save(model.state_dict(), model_save_path)\n","    print(f\"Model saved to {model_save_path}\")\n","\n","    print(\"Evaluating model...\")\n","    model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim).to(DEVICE)\n","    model.load_state_dict(torch.load(model_save_path))\n","    model.eval()  # Explicitly set evaluation mode\n","    true_labels, predictions = evaluate_model(model, test_loader)\n","\n","    print(\"\\nAccuracy:\", accuracy_score(true_labels, predictions))\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(true_labels, predictions))\n","\n","    print(\"\\nConfusion Matrix:\")\n","    print(confusion_matrix(true_labels, predictions))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Jx4cyjVl8t5","outputId":"31b58f07-90af-41a4-c56f-45342c8b4718"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Training with params: embed_dim=50, hidden_dim=128, n_layers=1, dropout=0.2, batch_size=32, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4326, Val Loss = 0.4074, Val Accuracy = 0.8135\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3934, Val Loss = 0.3980, Val Accuracy = 0.8184\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3792, Val Loss = 0.3964, Val Accuracy = 0.8203\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=1, dropout=0.2, batch_size=32, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4462, Val Loss = 0.4119, Val Accuracy = 0.8104\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3982, Val Loss = 0.3997, Val Accuracy = 0.8171\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3825, Val Loss = 0.3962, Val Accuracy = 0.8195\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=1, dropout=0.2, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4386, Val Loss = 0.4089, Val Accuracy = 0.8127\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3961, Val Loss = 0.4031, Val Accuracy = 0.8150\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3808, Val Loss = 0.3960, Val Accuracy = 0.8199\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=1, dropout=0.2, batch_size=64, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4578, Val Loss = 0.4198, Val Accuracy = 0.8058\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.4059, Val Loss = 0.4057, Val Accuracy = 0.8137\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3904, Val Loss = 0.3980, Val Accuracy = 0.8180\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=1, dropout=0.5, batch_size=32, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4316, Val Loss = 0.4081, Val Accuracy = 0.8133\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3922, Val Loss = 0.3967, Val Accuracy = 0.8188\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3780, Val Loss = 0.3956, Val Accuracy = 0.8197\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=1, dropout=0.5, batch_size=32, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4449, Val Loss = 0.4114, Val Accuracy = 0.8103\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3978, Val Loss = 0.3987, Val Accuracy = 0.8176\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3826, Val Loss = 0.3946, Val Accuracy = 0.8202\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=1, dropout=0.5, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4386, Val Loss = 0.4108, Val Accuracy = 0.8108\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3955, Val Loss = 0.3990, Val Accuracy = 0.8177\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3800, Val Loss = 0.3955, Val Accuracy = 0.8195\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=1, dropout=0.5, batch_size=64, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4573, Val Loss = 0.4177, Val Accuracy = 0.8073\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.4046, Val Loss = 0.4041, Val Accuracy = 0.8147\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3895, Val Loss = 0.3977, Val Accuracy = 0.8184\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=2, dropout=0.2, batch_size=32, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4320, Val Loss = 0.4056, Val Accuracy = 0.8144\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3922, Val Loss = 0.3962, Val Accuracy = 0.8197\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3782, Val Loss = 0.3920, Val Accuracy = 0.8223\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=2, dropout=0.2, batch_size=32, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4445, Val Loss = 0.4112, Val Accuracy = 0.8105\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3979, Val Loss = 0.3994, Val Accuracy = 0.8177\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3824, Val Loss = 0.3931, Val Accuracy = 0.8209\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=2, dropout=0.2, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4387, Val Loss = 0.4070, Val Accuracy = 0.8131\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3947, Val Loss = 0.3944, Val Accuracy = 0.8209\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3790, Val Loss = 0.3911, Val Accuracy = 0.8220\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=2, dropout=0.2, batch_size=64, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4540, Val Loss = 0.4173, Val Accuracy = 0.8076\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.4038, Val Loss = 0.4016, Val Accuracy = 0.8160\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3886, Val Loss = 0.3958, Val Accuracy = 0.8185\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=2, dropout=0.5, batch_size=32, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4353, Val Loss = 0.4069, Val Accuracy = 0.8136\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3982, Val Loss = 0.3964, Val Accuracy = 0.8193\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3858, Val Loss = 0.3930, Val Accuracy = 0.8210\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=2, dropout=0.5, batch_size=32, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4477, Val Loss = 0.4112, Val Accuracy = 0.8114\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.4025, Val Loss = 0.4001, Val Accuracy = 0.8168\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3889, Val Loss = 0.3950, Val Accuracy = 0.8198\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=2, dropout=0.5, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4403, Val Loss = 0.4076, Val Accuracy = 0.8135\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3995, Val Loss = 0.3972, Val Accuracy = 0.8193\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3863, Val Loss = 0.3940, Val Accuracy = 0.8213\n","Training with params: embed_dim=50, hidden_dim=128, n_layers=2, dropout=0.5, batch_size=64, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4594, Val Loss = 0.4175, Val Accuracy = 0.8075\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.4081, Val Loss = 0.4053, Val Accuracy = 0.8145\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3942, Val Loss = 0.3968, Val Accuracy = 0.8189\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=1, dropout=0.2, batch_size=32, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4300, Val Loss = 0.4030, Val Accuracy = 0.8159\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3889, Val Loss = 0.3944, Val Accuracy = 0.8201\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3721, Val Loss = 0.3956, Val Accuracy = 0.8211\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=1, dropout=0.2, batch_size=32, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4410, Val Loss = 0.4070, Val Accuracy = 0.8131\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3918, Val Loss = 0.3949, Val Accuracy = 0.8200\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3720, Val Loss = 0.3933, Val Accuracy = 0.8219\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=1, dropout=0.2, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4351, Val Loss = 0.4070, Val Accuracy = 0.8140\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3896, Val Loss = 0.3945, Val Accuracy = 0.8202\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3698, Val Loss = 0.3937, Val Accuracy = 0.8203\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=1, dropout=0.2, batch_size=64, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4513, Val Loss = 0.4120, Val Accuracy = 0.8102\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3973, Val Loss = 0.3979, Val Accuracy = 0.8178\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3782, Val Loss = 0.3934, Val Accuracy = 0.8211\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=1, dropout=0.5, batch_size=32, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4295, Val Loss = 0.4021, Val Accuracy = 0.8166\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3890, Val Loss = 0.3948, Val Accuracy = 0.8201\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3717, Val Loss = 0.3950, Val Accuracy = 0.8206\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=1, dropout=0.5, batch_size=32, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":["Training:  46%|████▌     | 18398/39921 [02:02<02:17, 156.50it/s]"]}],"source":["import os\n","import pickle\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from tqdm import tqdm\n","import itertools\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n","print(DEVICE)\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=100):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokenizer(self.texts[idx])\n","        tokens = tokens[:self.max_length]\n","        tokens = [0] * (self.max_length - len(tokens)) + tokens\n","\n","        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n","        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return tokens_tensor, label_tensor\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=1, dropout=0.2):\n","        super(LSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        hidden_state = lstm_out[:, -1, :]\n","        output = self.fc(hidden_state)\n","        return output\n","\n","def train_model(model, dataloader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0.0\n","    for inputs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n","        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    true_labels = []\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n","            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","\n","            preds = torch.argmax(outputs, dim=1).cpu().tolist()\n","            true_labels.extend(labels.cpu().tolist())\n","            predictions.extend(preds)\n","\n","    accuracy = accuracy_score(true_labels, predictions)\n","    return total_loss / len(dataloader), accuracy, true_labels, predictions\n","\n","if __name__ == \"__main__\":\n","    # Load preprocessed data and vectorizer\n","    vectorizer_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/vectorizer.pkl\"\n","    processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","\n","    with open(vectorizer_path, \"rb\") as f:\n","        vectorizer = pickle.load(f)\n","\n","    data = pd.read_csv(processed_data_path)\n","    data.dropna(subset=[\"text\"], inplace=True)\n","\n","    tokenizer_fn = vectorizer.build_analyzer()\n","\n","    def tokenizer(text):\n","        tokens = tokenizer_fn(text)\n","        return [vectorizer.vocabulary_.get(token, 0) for token in tokens]\n","\n","    train_texts, test_texts, train_labels, test_labels = train_test_split(\n","        data[\"text\"], data[\"sentiment\"], test_size=0.2, random_state=42\n","    )\n","\n","    train_dataset = SentimentDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n","    test_dataset = SentimentDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","    # Hyperparameter grid\n","    param_grid = {\n","        \"embed_dim\": [50, 100],\n","        \"hidden_dim\": [128, 256],\n","        \"n_layers\": [1, 2],\n","        \"dropout\": [0.2, 0.5],\n","        \"batch_size\": [32, 64],\n","        \"learning_rate\": [0.001, 0.0005]\n","    }\n","\n","    best_accuracy = 0\n","    best_params = {}\n","\n","    for params in itertools.product(*param_grid.values()):\n","        embed_dim, hidden_dim, n_layers, dropout, batch_size, learning_rate = params\n","\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","        vocab_size = len(vectorizer.vocabulary_)\n","        output_dim = 3\n","\n","        model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim, n_layers, dropout).to(DEVICE)\n","        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","        criterion = nn.CrossEntropyLoss()\n","\n","        print(f\"Training with params: embed_dim={embed_dim}, hidden_dim={hidden_dim}, n_layers={n_layers}, dropout={dropout}, batch_size={batch_size}, learning_rate={learning_rate}\")\n","\n","        for epoch in range(3):\n","            train_loss = train_model(model, train_loader, optimizer, criterion)\n","            val_loss, val_accuracy, _, _ = evaluate_model(model, test_loader, criterion)\n","            print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n","\n","            if val_accuracy > best_accuracy:\n","                best_accuracy = val_accuracy\n","                best_params = {\n","                    \"embed_dim\": embed_dim,\n","                    \"hidden_dim\": hidden_dim,\n","                    \"n_layers\": n_layers,\n","                    \"dropout\": dropout,\n","                    \"batch_size\": batch_size,\n","                    \"learning_rate\": learning_rate\n","                }\n","                torch.save(model.state_dict(), \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/tryouts/best_lstm_model.pt\")\n","\n","    print(\"Best Hyperparameters:\", best_params)\n","    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"em1MNOJZXng7","outputId":"068eee54-6c9c-41df-943b-cab49a1a595d"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Training with params: embed_dim=50, hidden_dim=256, n_layers=1, dropout=0.2, batch_size=32, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4299, Val Loss = 0.4038, Val Accuracy = 0.8147\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3890, Val Loss = 0.3961, Val Accuracy = 0.8197\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3722, Val Loss = 0.3945, Val Accuracy = 0.8209\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=1, dropout=0.2, batch_size=32, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4408, Val Loss = 0.4070, Val Accuracy = 0.8133\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3915, Val Loss = 0.3962, Val Accuracy = 0.8204\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3714, Val Loss = 0.3935, Val Accuracy = 0.8212\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=1, dropout=0.2, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4361, Val Loss = 0.4098, Val Accuracy = 0.8120\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3904, Val Loss = 0.3959, Val Accuracy = 0.8196\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3710, Val Loss = 0.3976, Val Accuracy = 0.8194\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=1, dropout=0.2, batch_size=64, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4523, Val Loss = 0.4133, Val Accuracy = 0.8097\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3988, Val Loss = 0.3993, Val Accuracy = 0.8175\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3796, Val Loss = 0.3939, Val Accuracy = 0.8202\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=1, dropout=0.5, batch_size=32, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4303, Val Loss = 0.4033, Val Accuracy = 0.8153\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3889, Val Loss = 0.3953, Val Accuracy = 0.8202\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3719, Val Loss = 0.3955, Val Accuracy = 0.8206\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=1, dropout=0.5, batch_size=32, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4425, Val Loss = 0.4096, Val Accuracy = 0.8127\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3919, Val Loss = 0.3962, Val Accuracy = 0.8190\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3717, Val Loss = 0.3943, Val Accuracy = 0.8213\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=1, dropout=0.5, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4359, Val Loss = 0.4054, Val Accuracy = 0.8139\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 2: Train Loss = 0.3902, Val Loss = 0.3951, Val Accuracy = 0.8205\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 3: Train Loss = 0.3711, Val Loss = 0.3934, Val Accuracy = 0.8213\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=1, dropout=0.5, batch_size=64, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4532, Val Loss = 0.4131, Val Accuracy = 0.8099\n"]},{"name":"stderr","output_type":"stream","text":["Training:  36%|███▋      | 7239/19961 [01:11<02:03, 102.71it/s]"]}],"source":["import os\n","import pickle\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from tqdm import tqdm\n","import itertools\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n","print(DEVICE)\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=100):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokenizer(self.texts[idx])\n","        tokens = tokens[:self.max_length]\n","        tokens = [0] * (self.max_length - len(tokens)) + tokens\n","\n","        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n","        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return tokens_tensor, label_tensor\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=1, dropout=0.2):\n","        super(LSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        hidden_state = lstm_out[:, -1, :]\n","        output = self.fc(hidden_state)\n","        return output\n","\n","def train_model(model, dataloader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0.0\n","    for inputs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n","        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    true_labels = []\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n","            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","\n","            preds = torch.argmax(outputs, dim=1).cpu().tolist()\n","            true_labels.extend(labels.cpu().tolist())\n","            predictions.extend(preds)\n","\n","    accuracy = accuracy_score(true_labels, predictions)\n","    return total_loss / len(dataloader), accuracy, true_labels, predictions\n","\n","if __name__ == \"__main__\":\n","    # Load preprocessed data and vectorizer\n","    vectorizer_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/vectorizer.pkl\"\n","    processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","\n","    with open(vectorizer_path, \"rb\") as f:\n","        vectorizer = pickle.load(f)\n","\n","    data = pd.read_csv(processed_data_path)\n","    data.dropna(subset=[\"text\"], inplace=True)\n","\n","    tokenizer_fn = vectorizer.build_analyzer()\n","\n","    def tokenizer(text):\n","        tokens = tokenizer_fn(text)\n","        return [vectorizer.vocabulary_.get(token, 0) for token in tokens]\n","\n","    train_texts, test_texts, train_labels, test_labels = train_test_split(\n","        data[\"text\"], data[\"sentiment\"], test_size=0.2, random_state=42\n","    )\n","\n","    train_dataset = SentimentDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n","    test_dataset = SentimentDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","    # Hyperparameter grid\n","    param_grid = {\n","        \"embed_dim\": [50, 100],\n","        \"hidden_dim\": [256],\n","        \"n_layers\": [1, 2],\n","        \"dropout\": [0.2, 0.5],\n","        \"batch_size\": [32, 64],\n","        \"learning_rate\": [0.001, 0.0005]\n","    }\n","\n","    best_accuracy = 0\n","    best_params = {}\n","\n","    for params in itertools.product(*param_grid.values()):\n","        embed_dim, hidden_dim, n_layers, dropout, batch_size, learning_rate = params\n","\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","        vocab_size = len(vectorizer.vocabulary_)\n","        output_dim = 3\n","\n","        model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim, n_layers, dropout).to(DEVICE)\n","        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","        criterion = nn.CrossEntropyLoss()\n","\n","        print(f\"Training with params: embed_dim={embed_dim}, hidden_dim={hidden_dim}, n_layers={n_layers}, dropout={dropout}, batch_size={batch_size}, learning_rate={learning_rate}\")\n","\n","        for epoch in range(3):\n","            train_loss = train_model(model, train_loader, optimizer, criterion)\n","            val_loss, val_accuracy, _, _ = evaluate_model(model, test_loader, criterion)\n","            print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n","\n","            if val_accuracy > best_accuracy:\n","                best_accuracy = val_accuracy\n","                best_params = {\n","                    \"embed_dim\": embed_dim,\n","                    \"hidden_dim\": hidden_dim,\n","                    \"n_layers\": n_layers,\n","                    \"dropout\": dropout,\n","                    \"batch_size\": batch_size,\n","                    \"learning_rate\": learning_rate\n","                }\n","                torch.save(model.state_dict(), \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/tryouts/best_lstm_model2.pt\")\n","\n","    print(\"Best Hyperparameters:\", best_params)\n","    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkWsIInZDnqG","outputId":"06d59606-6380-48f0-dea0-f582f5096a73"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Training with params: embed_dim=50, hidden_dim=256, n_layers=2, dropout=0.2, batch_size=32, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4302, Val Loss = 0.4043, Val Accuracy = 0.8161\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=2, dropout=0.2, batch_size=32, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4414, Val Loss = 0.4049, Val Accuracy = 0.8145\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=2, dropout=0.2, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4342, Val Loss = 0.4037, Val Accuracy = 0.8155\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=2, dropout=0.2, batch_size=64, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4501, Val Loss = 0.4112, Val Accuracy = 0.8112\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=2, dropout=0.5, batch_size=32, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4322, Val Loss = 0.4047, Val Accuracy = 0.8155\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=2, dropout=0.5, batch_size=32, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4443, Val Loss = 0.4074, Val Accuracy = 0.8125\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=2, dropout=0.5, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4373, Val Loss = 0.4041, Val Accuracy = 0.8152\n","Training with params: embed_dim=50, hidden_dim=256, n_layers=2, dropout=0.5, batch_size=64, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4536, Val Loss = 0.4152, Val Accuracy = 0.8084\n","Training with params: embed_dim=100, hidden_dim=256, n_layers=2, dropout=0.2, batch_size=32, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4231, Val Loss = 0.3981, Val Accuracy = 0.8185\n","Training with params: embed_dim=100, hidden_dim=256, n_layers=2, dropout=0.2, batch_size=32, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4302, Val Loss = 0.3994, Val Accuracy = 0.8171\n","Training with params: embed_dim=100, hidden_dim=256, n_layers=2, dropout=0.2, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4271, Val Loss = 0.4005, Val Accuracy = 0.8181\n","Training with params: embed_dim=100, hidden_dim=256, n_layers=2, dropout=0.2, batch_size=64, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":["Training:  67%|██████▋   | 13322/19961 [04:29<02:13, 49.63it/s]"]}],"source":["import os\n","import pickle\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from tqdm import tqdm\n","import itertools\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n","print(DEVICE)\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=100):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokenizer(self.texts[idx])\n","        tokens = tokens[:self.max_length]\n","        tokens = [0] * (self.max_length - len(tokens)) + tokens\n","\n","        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n","        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return tokens_tensor, label_tensor\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=1, dropout=0.2):\n","        super(LSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        hidden_state = lstm_out[:, -1, :]\n","        output = self.fc(hidden_state)\n","        return output\n","\n","def train_model(model, dataloader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0.0\n","    for inputs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n","        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    true_labels = []\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n","            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","\n","            preds = torch.argmax(outputs, dim=1).cpu().tolist()\n","            true_labels.extend(labels.cpu().tolist())\n","            predictions.extend(preds)\n","\n","    accuracy = accuracy_score(true_labels, predictions)\n","    return total_loss / len(dataloader), accuracy, true_labels, predictions\n","\n","if __name__ == \"__main__\":\n","    # Load preprocessed data and vectorizer\n","    vectorizer_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/vectorizer.pkl\"\n","    processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","\n","    with open(vectorizer_path, \"rb\") as f:\n","        vectorizer = pickle.load(f)\n","\n","    data = pd.read_csv(processed_data_path)\n","    data.dropna(subset=[\"text\"], inplace=True)\n","\n","    tokenizer_fn = vectorizer.build_analyzer()\n","\n","    def tokenizer(text):\n","        tokens = tokenizer_fn(text)\n","        return [vectorizer.vocabulary_.get(token, 0) for token in tokens]\n","\n","    train_texts, test_texts, train_labels, test_labels = train_test_split(\n","        data[\"text\"], data[\"sentiment\"], test_size=0.2, random_state=42\n","    )\n","\n","    train_dataset = SentimentDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n","    test_dataset = SentimentDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","    # Hyperparameter grid\n","    param_grid = {\n","        \"embed_dim\": [50, 100],\n","        \"hidden_dim\": [256],\n","        \"n_layers\": [2],\n","        \"dropout\": [0.2, 0.5],\n","        \"batch_size\": [32, 64],\n","        \"learning_rate\": [0.001, 0.0005]\n","    }\n","\n","    best_accuracy = 0\n","    best_params = {}\n","\n","    for params in itertools.product(*param_grid.values()):\n","        embed_dim, hidden_dim, n_layers, dropout, batch_size, learning_rate = params\n","\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","        vocab_size = len(vectorizer.vocabulary_)\n","        output_dim = 3\n","\n","        model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim, n_layers, dropout).to(DEVICE)\n","        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","        criterion = nn.CrossEntropyLoss()\n","\n","        print(f\"Training with params: embed_dim={embed_dim}, hidden_dim={hidden_dim}, n_layers={n_layers}, dropout={dropout}, batch_size={batch_size}, learning_rate={learning_rate}\")\n","\n","        for epoch in range(1):\n","            train_loss = train_model(model, train_loader, optimizer, criterion)\n","            val_loss, val_accuracy, _, _ = evaluate_model(model, test_loader, criterion)\n","            print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n","\n","            if val_accuracy > best_accuracy:\n","                best_accuracy = val_accuracy\n","                best_params = {\n","                    \"embed_dim\": embed_dim,\n","                    \"hidden_dim\": hidden_dim,\n","                    \"n_layers\": n_layers,\n","                    \"dropout\": dropout,\n","                    \"batch_size\": batch_size,\n","                    \"learning_rate\": learning_rate\n","                }\n","                torch.save(model.state_dict(), \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/tryouts/best_lstm_model_n_layers_2.pt\")\n","\n","    print(\"Best Hyperparameters:\", best_params)\n","    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":847748,"status":"ok","timestamp":1736854060332,"user":{"displayName":"Buğra","userId":"13429055383207080256"},"user_tz":-60},"id":"FFC6DHqIVbP4","outputId":"10140a65-a05f-4937-a416-0b3ce0657980"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Training with params: embed_dim=100, hidden_dim=256, n_layers=2, dropout=0.2, batch_size=32, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4231, Val Loss = 0.4003, Val Accuracy = 0.8173\n","Training with params: embed_dim=100, hidden_dim=256, n_layers=2, dropout=0.2, batch_size=32, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4302, Val Loss = 0.3990, Val Accuracy = 0.8179\n","Training with params: embed_dim=100, hidden_dim=256, n_layers=2, dropout=0.2, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4267, Val Loss = 0.3976, Val Accuracy = 0.8187\n","Training with params: embed_dim=100, hidden_dim=256, n_layers=2, dropout=0.2, batch_size=64, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4375, Val Loss = 0.4030, Val Accuracy = 0.8145\n","Training with params: embed_dim=100, hidden_dim=256, n_layers=2, dropout=0.5, batch_size=32, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4266, Val Loss = 0.4009, Val Accuracy = 0.8170\n","Training with params: embed_dim=100, hidden_dim=256, n_layers=2, dropout=0.5, batch_size=32, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4337, Val Loss = 0.4025, Val Accuracy = 0.8173\n","Training with params: embed_dim=100, hidden_dim=256, n_layers=2, dropout=0.5, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4298, Val Loss = 0.4002, Val Accuracy = 0.8174\n","Training with params: embed_dim=100, hidden_dim=256, n_layers=2, dropout=0.5, batch_size=64, learning_rate=0.0005\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4406, Val Loss = 0.4048, Val Accuracy = 0.8142\n","Best Hyperparameters: {'embed_dim': 100, 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.2, 'batch_size': 64, 'learning_rate': 0.001}\n","Best Accuracy: 0.8187\n"]}],"source":["import os\n","import pickle\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from tqdm import tqdm\n","import itertools\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n","print(DEVICE)\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=100):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokenizer(self.texts[idx])\n","        tokens = tokens[:self.max_length]\n","        tokens = [0] * (self.max_length - len(tokens)) + tokens\n","\n","        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n","        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return tokens_tensor, label_tensor\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=1, dropout=0.2):\n","        super(LSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        hidden_state = lstm_out[:, -1, :]\n","        output = self.fc(hidden_state)\n","        return output\n","\n","def train_model(model, dataloader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0.0\n","    for inputs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n","        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    true_labels = []\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n","            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","\n","            preds = torch.argmax(outputs, dim=1).cpu().tolist()\n","            true_labels.extend(labels.cpu().tolist())\n","            predictions.extend(preds)\n","\n","    accuracy = accuracy_score(true_labels, predictions)\n","    return total_loss / len(dataloader), accuracy, true_labels, predictions\n","\n","if __name__ == \"__main__\":\n","    # Load preprocessed data and vectorizer\n","    vectorizer_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/vectorizer.pkl\"\n","    processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","\n","    with open(vectorizer_path, \"rb\") as f:\n","        vectorizer = pickle.load(f)\n","\n","    data = pd.read_csv(processed_data_path)\n","    data.dropna(subset=[\"text\"], inplace=True)\n","\n","    tokenizer_fn = vectorizer.build_analyzer()\n","\n","    def tokenizer(text):\n","        tokens = tokenizer_fn(text)\n","        return [vectorizer.vocabulary_.get(token, 0) for token in tokens]\n","\n","    train_texts, test_texts, train_labels, test_labels = train_test_split(\n","        data[\"text\"], data[\"sentiment\"], test_size=0.2, random_state=42\n","    )\n","\n","    train_dataset = SentimentDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n","    test_dataset = SentimentDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","    # Hyperparameter grid\n","    param_grid = {\n","        \"embed_dim\": [100],\n","        \"hidden_dim\": [256],\n","        \"n_layers\": [2],\n","        \"dropout\": [0.2, 0.5],\n","        \"batch_size\": [32, 64],\n","        \"learning_rate\": [0.001, 0.0005]\n","    }\n","\n","    best_accuracy = 0\n","    best_params = {}\n","\n","    for params in itertools.product(*param_grid.values()):\n","        embed_dim, hidden_dim, n_layers, dropout, batch_size, learning_rate = params\n","\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","        vocab_size = len(vectorizer.vocabulary_)\n","        output_dim = 3\n","\n","        model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim, n_layers, dropout).to(DEVICE)\n","        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","        criterion = nn.CrossEntropyLoss()\n","\n","        print(f\"Training with params: embed_dim={embed_dim}, hidden_dim={hidden_dim}, n_layers={n_layers}, dropout={dropout}, batch_size={batch_size}, learning_rate={learning_rate}\")\n","\n","        for epoch in range(1):\n","            train_loss = train_model(model, train_loader, optimizer, criterion)\n","            val_loss, val_accuracy, _, _ = evaluate_model(model, test_loader, criterion)\n","            print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n","\n","            if val_accuracy > best_accuracy:\n","                best_accuracy = val_accuracy\n","                best_params = {\n","                    \"embed_dim\": embed_dim,\n","                    \"hidden_dim\": hidden_dim,\n","                    \"n_layers\": n_layers,\n","                    \"dropout\": dropout,\n","                    \"batch_size\": batch_size,\n","                    \"learning_rate\": learning_rate\n","                }\n","                torch.save(model.state_dict(), \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/tryouts/best_lstm_model_embed_dim_100.pt\")\n","\n","    print(\"Best Hyperparameters:\", best_params)\n","    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"sTQBpZ84kvme","outputId":"9d62f097-f56e-4fed-f599-9b4b4d1e355f"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Training with params: embed_dim=100, hidden_dim=256, n_layers=1, dropout=0.2, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4266, Val Loss = 0.4000, Val Accuracy = 0.8179\n","Training with params: embed_dim=100, hidden_dim=256, n_layers=1, dropout=0.5, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4276, Val Loss = 0.4010, Val Accuracy = 0.8167\n","Training with params: embed_dim=100, hidden_dim=256, n_layers=2, dropout=0.2, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4261, Val Loss = 0.3973, Val Accuracy = 0.8189\n","Training with params: embed_dim=100, hidden_dim=256, n_layers=2, dropout=0.5, batch_size=64, learning_rate=0.001\n"]},{"name":"stderr","output_type":"stream","text":[""]},{"name":"stdout","output_type":"stream","text":["Epoch 1: Train Loss = 0.4292, Val Loss = 0.3991, Val Accuracy = 0.8176\n","Best Hyperparameters: {'embed_dim': 100, 'hidden_dim': 256, 'n_layers': 2, 'dropout': 0.2, 'batch_size': 64, 'learning_rate': 0.001}\n","Best Accuracy: 0.8189\n"]}],"source":["3import os\n","import pickle\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","import pandas as pd\n","from tqdm import tqdm\n","import itertools\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n","print(DEVICE)\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=100):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokenizer(self.texts[idx])\n","        tokens = tokens[:self.max_length]\n","        tokens = [0] * (self.max_length - len(tokens)) + tokens\n","\n","        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n","        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return tokens_tensor, label_tensor\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=1, dropout=0.2):\n","        super(LSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        hidden_state = lstm_out[:, -1, :]\n","        output = self.fc(hidden_state)\n","        return output\n","\n","def train_model(model, dataloader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0.0\n","    for inputs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n","        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    true_labels = []\n","    predictions = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n","            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","\n","            preds = torch.argmax(outputs, dim=1).cpu().tolist()\n","            true_labels.extend(labels.cpu().tolist())\n","            predictions.extend(preds)\n","\n","    accuracy = accuracy_score(true_labels, predictions)\n","    return total_loss / len(dataloader), accuracy, true_labels, predictions\n","\n","if __name__ == \"__main__\":\n","    # Load preprocessed data and vectorizer\n","    vectorizer_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/vectorizer.pkl\"\n","    processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","\n","    with open(vectorizer_path, \"rb\") as f:\n","        vectorizer = pickle.load(f)\n","\n","    data = pd.read_csv(processed_data_path)\n","    data.dropna(subset=[\"text\"], inplace=True)\n","\n","    tokenizer_fn = vectorizer.build_analyzer()\n","\n","    def tokenizer(text):\n","        tokens = tokenizer_fn(text)\n","        return [vectorizer.vocabulary_.get(token, 0) for token in tokens]\n","\n","    train_texts, test_texts, train_labels, test_labels = train_test_split(\n","        data[\"text\"], data[\"sentiment\"], test_size=0.2, random_state=42\n","    )\n","\n","    train_dataset = SentimentDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n","    test_dataset = SentimentDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","    # Hyperparameter grid\n","    param_grid = {\n","        \"embed_dim\": [100],\n","        \"hidden_dim\": [256],\n","        \"n_layers\": [1, 2],\n","        \"dropout\": [0.2, 0.5],\n","        \"batch_size\": [64],\n","        \"learning_rate\": [0.001]\n","    }\n","\n","    best_accuracy = 0\n","    best_params = {}\n","\n","    for params in itertools.product(*param_grid.values()):\n","        embed_dim, hidden_dim, n_layers, dropout, batch_size, learning_rate = params\n","\n","        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","        test_loader = DataLoader(test_dataset, batch_size=batch_size)\n","\n","        vocab_size = len(vectorizer.vocabulary_)\n","        output_dim = 3\n","\n","        model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim, n_layers, dropout).to(DEVICE)\n","        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","        criterion = nn.CrossEntropyLoss()\n","\n","        print(f\"Training with params: embed_dim={embed_dim}, hidden_dim={hidden_dim}, n_layers={n_layers}, dropout={dropout}, batch_size={batch_size}, learning_rate={learning_rate}\")\n","\n","        for epoch in range(1):\n","            train_loss = train_model(model, train_loader, optimizer, criterion)\n","            val_loss, val_accuracy, _, _ = evaluate_model(model, test_loader, criterion)\n","            print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n","\n","            if val_accuracy > best_accuracy:\n","                best_accuracy = val_accuracy\n","                best_params = {\n","                    \"embed_dim\": embed_dim,\n","                    \"hidden_dim\": hidden_dim,\n","                    \"n_layers\": n_layers,\n","                    \"dropout\": dropout,\n","                    \"batch_size\": batch_size,\n","                    \"learning_rate\": learning_rate\n","                }\n","                torch.save(model.state_dict(), \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/tryouts/best_lstm_model_n_layers.pt\")\n","\n","    print(\"Best Hyperparameters:\", best_params)\n","    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n"]},{"cell_type":"markdown","source":["# Hypermeter Try-out LSTM Models\n","##### `different hyperparameters have been tried to find the best accuracy, they are not used in the final version of the project`"],"metadata":{"id":"XYtWziNZDL6b"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"PPDWkqMOnbqo","outputId":"b9cec3dc-2670-4325-ea50-0ec2f22cf60b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading vectorizer...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Loading preprocessed data...\n","Splitting data into train and test sets...\n","Initializing model...\n","Using device: cuda\n","Training model...\n","Epoch 1: Train Loss = 0.4283, Val Loss = 0.3997, Val Accuracy = 0.8177\n","Epoch 2: Train Loss = 0.3812, Val Loss = 0.3898, Val Accuracy = 0.8229\n","Epoch 3: Train Loss = 0.3574, Val Loss = 0.3902, Val Accuracy = 0.8228\n","Epoch 4: Train Loss = 0.3357, Val Loss = 0.3977, Val Accuracy = 0.8215\n","Epoch 5: Train Loss = 0.3159, Val Loss = 0.4075, Val Accuracy = 0.8196\n","Epoch 6: Train Loss = 0.2993, Val Loss = 0.4137, Val Accuracy = 0.8163\n","Epoch 7: Train Loss = 0.2863, Val Loss = 0.4307, Val Accuracy = 0.8153\n","Epoch 8: Train Loss = 0.2765, Val Loss = 0.4386, Val Accuracy = 0.8132\n","Epoch 9: Train Loss = 0.2689, Val Loss = 0.4509, Val Accuracy = 0.8126\n","Epoch 10: Train Loss = 0.2641, Val Loss = 0.4515, Val Accuracy = 0.8112\n","Saving model...\n","Model saved to /content/drive/MyDrive/Colab Notebooks/SentimentAnalysis/wout_metadata/lstm_model_10_epochs.pt\n","Evaluating model...\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-2-fcb83038c145>:141: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_save_path))\n"]},{"ename":"TypeError","evalue":"evaluate_model() missing 1 required positional argument: 'criterion'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-fcb83038c145>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Explicitly set evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nAccuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: evaluate_model() missing 1 required positional argument: 'criterion'"]}],"source":["import os\n","import pickle\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import pandas as pd\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n","\n","seed = 42\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","if torch.backends.cudnn.is_available():\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=100):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokenizer(self.texts[idx])\n","        tokens = tokens[:self.max_length]  # Truncate if too long\n","        tokens = [0] * (self.max_length - len(tokens)) + tokens  # Pad if too short\n","\n","        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n","        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return tokens_tensor, label_tensor\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=2):\n","        super(LSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        hidden_state = lstm_out[:, -1, :]\n","        output = self.fc(hidden_state)\n","        return output\n","\n","def train_model(model, dataloader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0.0\n","    for inputs, labels in dataloader:\n","        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    true_labels, predictions = [], []\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","            predictions.extend(preds)\n","            true_labels.extend(labels.cpu().numpy())\n","    #return true_labels, predictions\n","    accuracy = accuracy_score(true_labels, predictions)\n","    return total_loss / len(dataloader), accuracy, true_labels, predictions\n","\n","if __name__ == \"__main__\":\n","    vectorizer_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/vectorizer.pkl\"\n","    processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","    model_save_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/tryouts/lstm_model_10_epochs.pt\"\n","\n","    print(\"Loading vectorizer...\")\n","    with open(vectorizer_path, \"rb\") as f:\n","        vectorizer = pickle.load(f)\n","\n","    print(\"Loading preprocessed data...\")\n","    data = pd.read_csv(processed_data_path)\n","    data.dropna(subset=[\"text\"], inplace=True)\n","\n","    tokenizer_fn = vectorizer.build_analyzer()\n","\n","    def tokenizer(text):\n","        tokens = tokenizer_fn(text)\n","        return [vectorizer.vocabulary_.get(token, 0) for token in tokens]\n","\n","    print(\"Splitting data into train and test sets...\")\n","    train_texts, test_texts, train_labels, test_labels = train_test_split(\n","        data[\"text\"], data[\"sentiment\"], test_size=0.3, random_state=42\n","    )\n","\n","    train_dataset = SentimentDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n","    test_dataset = SentimentDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=64)\n","\n","    vocab_size = len(vectorizer.vocabulary_)\n","    embed_dim = 100\n","    hidden_dim = 256\n","    output_dim = 3\n","\n","    print(\"Initializing model...\")\n","    model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim).to(DEVICE)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    print(f\"Using device: {DEVICE}\")\n","\n","    print(\"Training model...\")\n","    for epoch in range(10):\n","        train_loss = train_model(model, train_loader, optimizer, criterion)\n","        val_loss, val_accuracy, _, _ = evaluate_model(model, test_loader, criterion)\n","        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n","\n","    print(\"Saving model...\")\n","    os.makedirs(\"models\", exist_ok=True)\n","    torch.save(model.state_dict(), model_save_path)\n","    print(f\"Model saved to {model_save_path}\")\n","\n","    print(\"Evaluating model...\")\n","    model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim).to(DEVICE)\n","    model.load_state_dict(torch.load(model_save_path))\n","    model.eval()  # Explicitly set evaluation mode\n","    true_labels, predictions = evaluate_model(model, test_loader)\n","\n","    print(\"\\nAccuracy:\", accuracy_score(true_labels, predictions))\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(true_labels, predictions))\n","\n","    print(\"\\nConfusion Matrix:\")\n","    print(confusion_matrix(true_labels, predictions))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":227824,"status":"ok","timestamp":1737533187367,"user":{"displayName":"Buğra","userId":"13429055383207080256"},"user_tz":-60},"id":"27rvpouX-k0E","outputId":"4527228f-55a1-4983-898a-e5bbccb70f29"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading vectorizer...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Loading preprocessed data...\n","Preparing test dataset...\n","Loading model...\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-2-5ee5b91350b5>:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_save_path))\n"]},{"name":"stdout","output_type":"stream","text":["Evaluating model...\n","\n","Test Accuracy: 0.8766706537327079\n","\n","Accuracy: 0.8766706537327079\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.87      0.88      0.88    798396\n","     Neutral       0.88      0.87      0.88    798434\n","\n","    accuracy                           0.88   1596830\n","   macro avg       0.88      0.88      0.88   1596830\n","weighted avg       0.88      0.88      0.88   1596830\n","\n","\n","Confusion Matrix:\n","[[703287  95109]\n"," [101827 696607]]\n"]}],"source":["import os\n","import pickle\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import pandas as pd\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Set random seed for reproducibility\n","seed = 42\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","if torch.backends.cudnn.is_available():\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=100):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokenizer(self.texts[idx])\n","        tokens = tokens[:self.max_length]\n","        tokens = [0] * (self.max_length - len(tokens)) + tokens\n","\n","        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n","        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return tokens_tensor, label_tensor\n","\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=2):\n","        super(LSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        hidden_state = lstm_out[:, -1, :]\n","        output = self.fc(hidden_state)\n","        return output\n","\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    true_labels, predictions = [], []\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","            predictions.extend(preds)\n","            true_labels.extend(labels.cpu().numpy())\n","    accuracy = accuracy_score(true_labels, predictions)\n","    return total_loss / len(dataloader), accuracy, true_labels, predictions\n","\n","\n","if __name__ == \"__main__\":\n","    vectorizer_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/vectorizer.pkl\"\n","    processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","    model_save_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/tryouts/lstm_model_10_epochs.pt\"\n","\n","    print(\"Loading vectorizer...\")\n","    with open(vectorizer_path, \"rb\") as f:\n","        vectorizer = pickle.load(f)\n","\n","    print(\"Loading preprocessed data...\")\n","    data = pd.read_csv(processed_data_path)\n","    data.dropna(subset=[\"text\"], inplace=True)\n","\n","    tokenizer_fn = vectorizer.build_analyzer()\n","\n","    def tokenizer(text):\n","        tokens = tokenizer_fn(text)\n","        return [vectorizer.vocabulary_.get(token, 0) for token in tokens]\n","\n","    print(\"Preparing test dataset...\")\n","    test_texts, test_labels = data[\"text\"].tolist(), data[\"sentiment\"].tolist()\n","    test_dataset = SentimentDataset(test_texts, test_labels, tokenizer)\n","    test_loader = DataLoader(test_dataset, batch_size=64)\n","\n","    vocab_size = len(vectorizer.vocabulary_)\n","    embed_dim = 100\n","    hidden_dim = 256\n","    output_dim = 3\n","\n","    print(\"Loading model...\")\n","    model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim).to(DEVICE)\n","    model.load_state_dict(torch.load(model_save_path))\n","\n","    print(\"Evaluating model...\")\n","    criterion = nn.CrossEntropyLoss()\n","    _, test_accuracy, true_labels, predictions = evaluate_model(model, test_loader, criterion)\n","\n","    unique_classes = sorted(set(true_labels))\n","    num_classes = len(unique_classes)\n","\n","    # Generate target names based on the number of classes\n","    default_target_names = [\"Class \" + str(i) for i in range(num_classes)]\n","    custom_target_names = [\"Negative\", \"Neutral\", \"Positive\"][:num_classes]  # Adjust to match detected classes\n","\n","    print(\"\\nTest Accuracy:\", test_accuracy)\n","    print(\"\\nAccuracy:\", accuracy_score(true_labels, predictions))\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(true_labels, predictions, target_names=custom_target_names))\n","\n","    print(\"\\nConfusion Matrix:\")\n","    print(confusion_matrix(true_labels, predictions, labels=unique_classes))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"bRDOIBH6TsS_","outputId":"90fb782f-8634-4769-8707-167534f4a178"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading vectorizer...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Loading preprocessed data...\n","Splitting data into train and test sets...\n","Initializing model...\n","Using device: cuda\n","Training model...\n","Epoch 1: Train Loss = 0.4326, Val Loss = 0.4095, Val Accuracy = 0.8120\n","Epoch 2: Train Loss = 0.3992, Val Loss = 0.4000, Val Accuracy = 0.8177\n","Epoch 3: Train Loss = 0.3895, Val Loss = 0.3948, Val Accuracy = 0.8197\n","Epoch 4: Train Loss = 0.3840, Val Loss = 0.3924, Val Accuracy = 0.8214\n","Epoch 5: Train Loss = 0.3799, Val Loss = 0.3921, Val Accuracy = 0.8218\n","Epoch 6: Train Loss = 0.3765, Val Loss = 0.3901, Val Accuracy = 0.8233\n","Epoch 7: Train Loss = 0.3740, Val Loss = 0.3907, Val Accuracy = 0.8223\n","Saving model...\n","Model saved to /content/drive/MyDrive/Colab Notebooks/SentimentAnalysis/wout_metadata/lstm_model_7_epochs.pt\n","Evaluating model...\n","\n","Test Accuracy: 0.8223057557786364\n","\n","Accuracy: 0.8223057557786364\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.82      0.83      0.82    319015\n","     Neutral       0.83      0.82      0.82    319717\n","\n","    accuracy                           0.82    638732\n","   macro avg       0.82      0.82      0.82    638732\n","weighted avg       0.82      0.82      0.82    638732\n","\n","\n","Confusion Matrix:\n","[[264483  54532]\n"," [ 58967 260750]]\n"]}],"source":["import os\n","import pickle\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import pandas as pd\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n","\n","seed = 42\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","if torch.backends.cudnn.is_available():\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=100):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokenizer(self.texts[idx])\n","        tokens = tokens[:self.max_length]  # Truncate if too long\n","        tokens = [0] * (self.max_length - len(tokens)) + tokens  # Pad if too short\n","\n","        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n","        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return tokens_tensor, label_tensor\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=1):\n","        super(LSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        hidden_state = lstm_out[:, -1, :]\n","        output = self.fc(hidden_state)\n","        return output\n","\n","def train_model(model, dataloader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0.0\n","    for inputs, labels in dataloader:\n","        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    true_labels, predictions = [], []\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","            predictions.extend(preds)\n","            true_labels.extend(labels.cpu().numpy())\n","    #return true_labels, predictions\n","    accuracy = accuracy_score(true_labels, predictions)\n","    return total_loss / len(dataloader), accuracy, true_labels, predictions\n","\n","if __name__ == \"__main__\":\n","    vectorizer_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/vectorizer.pkl\"\n","    processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","    model_save_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/tryouts/lstm_model_7_epochs.pt\"\n","\n","    print(\"Loading vectorizer...\")\n","    with open(vectorizer_path, \"rb\") as f:\n","        vectorizer = pickle.load(f)\n","\n","    print(\"Loading preprocessed data...\")\n","    data = pd.read_csv(processed_data_path)\n","    data.dropna(subset=[\"text\"], inplace=True)\n","\n","    tokenizer_fn = vectorizer.build_analyzer()\n","\n","    def tokenizer(text):\n","        tokens = tokenizer_fn(text)\n","        return [vectorizer.vocabulary_.get(token, 0) for token in tokens]\n","\n","    print(\"Splitting data into train and test sets...\")\n","    train_texts, test_texts, train_labels, test_labels = train_test_split(\n","        data[\"text\"], data[\"sentiment\"], test_size=0.4, random_state=42\n","    )\n","\n","    train_dataset = SentimentDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n","    test_dataset = SentimentDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","    vocab_size = len(vectorizer.vocabulary_)\n","    embed_dim = 100\n","    hidden_dim = 128\n","    output_dim = 3\n","\n","    print(\"Initializing model...\")\n","    model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim).to(DEVICE)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    print(f\"Using device: {DEVICE}\")\n","\n","    print(\"Training model...\")\n","    for epoch in range(7):\n","        train_loss = train_model(model, train_loader, optimizer, criterion)\n","        val_loss, val_accuracy, _, _ = evaluate_model(model, test_loader, criterion)\n","        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n","\n","    print(\"Saving model...\")\n","    os.makedirs(\"models\", exist_ok=True)\n","    torch.save(model.state_dict(), model_save_path)\n","    print(f\"Model saved to {model_save_path}\")\n","\n","    print(\"Evaluating model...\")\n","    criterion = nn.CrossEntropyLoss()\n","    _, test_accuracy, true_labels, predictions = evaluate_model(model, test_loader, criterion)\n","\n","    unique_classes = sorted(set(true_labels))\n","    num_classes = len(unique_classes)\n","\n","    # Generate target names based on the number of classes\n","    default_target_names = [\"Class \" + str(i) for i in range(num_classes)]\n","    custom_target_names = [\"Negative\", \"Neutral\", \"Positive\"][:num_classes]  # Adjust to match detected classes\n","\n","    print(\"\\nTest Accuracy:\", test_accuracy)\n","    print(\"\\nAccuracy:\", accuracy_score(true_labels, predictions))\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(true_labels, predictions, target_names=custom_target_names))\n","\n","    print(\"\\nConfusion Matrix:\")\n","    print(confusion_matrix(true_labels, predictions))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":132234,"status":"ok","timestamp":1737556485081,"user":{"displayName":"Buğra","userId":"13429055383207080256"},"user_tz":-60},"id":"yp-8ZWykbOgF","outputId":"13cd2676-2aeb-4992-8bf2-825932f0f271"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading vectorizer...\n","Loading preprocessed data...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Preparing test dataset...\n","Loading model...\n","Evaluating model...\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-3-7bfc6e727325>:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_save_path))\n"]},{"name":"stdout","output_type":"stream","text":["\n","Accuracy: 0.8337581333016038\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.83      0.84      0.83    798396\n","     Neutral       0.84      0.83      0.83    798434\n","\n","    accuracy                           0.83   1596830\n","   macro avg       0.83      0.83      0.83   1596830\n","weighted avg       0.83      0.83      0.83   1596830\n","\n","\n","Confusion Matrix:\n","[[671233 127163]\n"," [138297 660137]]\n"]}],"source":["import os\n","import pickle\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import pandas as pd\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Set random seed for reproducibility\n","seed = 42\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","if torch.backends.cudnn.is_available():\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=100):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokenizer(self.texts[idx])\n","        tokens = tokens[:self.max_length]\n","        tokens = [0] * (self.max_length - len(tokens)) + tokens\n","\n","        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n","        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return tokens_tensor, label_tensor\n","\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=1):\n","        super(LSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        hidden_state = lstm_out[:, -1, :]\n","        output = self.fc(hidden_state)\n","        return output\n","\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    true_labels, predictions = [], []\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","            predictions.extend(preds)\n","            true_labels.extend(labels.cpu().numpy())\n","    accuracy = accuracy_score(true_labels, predictions)\n","    return total_loss / len(dataloader), accuracy, true_labels, predictions\n","\n","\n","if __name__ == \"__main__\":\n","    vectorizer_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/vectorizer.pkl\"\n","    processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","    model_save_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/tryouts/lstm_model_7_epochs.pt\"\n","\n","    print(\"Loading vectorizer...\")\n","    with open(vectorizer_path, \"rb\") as f:\n","        vectorizer = pickle.load(f)\n","\n","    print(\"Loading preprocessed data...\")\n","    data = pd.read_csv(processed_data_path)\n","    data.dropna(subset=[\"text\"], inplace=True)\n","\n","    tokenizer_fn = vectorizer.build_analyzer()\n","\n","    def tokenizer(text):\n","        tokens = tokenizer_fn(text)\n","        return [vectorizer.vocabulary_.get(token, 0) for token in tokens]\n","\n","    print(\"Preparing test dataset...\")\n","    test_texts, test_labels = data[\"text\"].tolist(), data[\"sentiment\"].tolist()\n","    test_dataset = SentimentDataset(test_texts, test_labels, tokenizer)\n","    test_loader = DataLoader(test_dataset, batch_size=32)\n","\n","    vocab_size = len(vectorizer.vocabulary_)\n","    embed_dim = 100\n","    hidden_dim = 128\n","    output_dim = 3\n","\n","    print(\"Loading model...\")\n","    model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim).to(DEVICE)\n","    model.load_state_dict(torch.load(model_save_path))\n","\n","    print(\"Evaluating model...\")\n","    criterion = nn.CrossEntropyLoss()\n","    _, test_accuracy, true_labels, predictions = evaluate_model(model, test_loader, criterion)\n","\n","    unique_classes = sorted(set(true_labels))\n","    num_classes = len(unique_classes)\n","\n","    # Generate target names based on the number of classes\n","    default_target_names = [\"Class \" + str(i) for i in range(num_classes)]\n","    custom_target_names = [\"Negative\", \"Neutral\", \"Positive\"][:num_classes]  # Adjust to match detected classes\n","\n","    print(\"\\nAccuracy:\", accuracy_score(true_labels, predictions))\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(true_labels, predictions, target_names=custom_target_names))\n","\n","    print(\"\\nConfusion Matrix:\")\n","    print(confusion_matrix(true_labels, predictions, labels=unique_classes))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"lLVYZGu3fDVS","outputId":"3ed22b25-5ce3-48a7-c976-6f33be107b37"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading vectorizer...\n","Loading preprocessed data...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Splitting data into train and test sets...\n","Initializing model...\n","Using device: cuda\n","Training model...\n","Epoch 1: Train Loss = 0.4308, Val Loss = 0.4056, Val Accuracy = 0.8141\n","Epoch 2: Train Loss = 0.3908, Val Loss = 0.3976, Val Accuracy = 0.8187\n","Epoch 3: Train Loss = 0.3745, Val Loss = 0.3937, Val Accuracy = 0.8211\n","Epoch 4: Train Loss = 0.3609, Val Loss = 0.3965, Val Accuracy = 0.8202\n","Epoch 5: Train Loss = 0.3485, Val Loss = 0.3993, Val Accuracy = 0.8195\n","Epoch 6: Train Loss = 0.3373, Val Loss = 0.4063, Val Accuracy = 0.8179\n","Epoch 7: Train Loss = 0.3267, Val Loss = 0.4121, Val Accuracy = 0.8173\n","Saving model...\n","Model saved to /content/drive/MyDrive/Colab Notebooks/SentimentAnalysis/wout_metadata/lstm_model_7_epochs_batch_64.pt\n","Evaluating model...\n","\n","Test Accuracy: 0.8173099202795538\n","\n","Accuracy: 0.8173099202795538\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.81      0.82      0.82    159215\n","     Neutral       0.82      0.81      0.82    160151\n","\n","    accuracy                           0.82    319366\n","   macro avg       0.82      0.82      0.82    319366\n","weighted avg       0.82      0.82      0.82    319366\n","\n","\n","Confusion Matrix:\n","[[130860  28355]\n"," [ 29990 130161]]\n"]}],"source":["import os\n","import pickle\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import pandas as pd\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n","\n","seed = 42\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","if torch.backends.cudnn.is_available():\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=100):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokenizer(self.texts[idx])\n","        tokens = tokens[:self.max_length]  # Truncate if too long\n","        tokens = [0] * (self.max_length - len(tokens)) + tokens  # Pad if too short\n","\n","        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n","        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return tokens_tensor, label_tensor\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=1):\n","        super(LSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        hidden_state = lstm_out[:, -1, :]\n","        output = self.fc(hidden_state)\n","        return output\n","\n","def train_model(model, dataloader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0.0\n","    for inputs, labels in dataloader:\n","        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    true_labels, predictions = [], []\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","            predictions.extend(preds)\n","            true_labels.extend(labels.cpu().numpy())\n","    #return true_labels, predictions\n","    accuracy = accuracy_score(true_labels, predictions)\n","    return total_loss / len(dataloader), accuracy, true_labels, predictions\n","\n","if __name__ == \"__main__\":\n","    vectorizer_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/vectorizer.pkl\"\n","    processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","    model_save_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/tryouts/lstm_model_7_epochs_batch_64.pt\"\n","\n","    print(\"Loading vectorizer...\")\n","    with open(vectorizer_path, \"rb\") as f:\n","        vectorizer = pickle.load(f)\n","\n","    print(\"Loading preprocessed data...\")\n","    data = pd.read_csv(processed_data_path)\n","    data.dropna(subset=[\"text\"], inplace=True)\n","\n","    tokenizer_fn = vectorizer.build_analyzer()\n","\n","    def tokenizer(text):\n","        tokens = tokenizer_fn(text)\n","        return [vectorizer.vocabulary_.get(token, 0) for token in tokens]\n","\n","    print(\"Splitting data into train and test sets...\")\n","    train_texts, test_texts, train_labels, test_labels = train_test_split(\n","        data[\"text\"], data[\"sentiment\"], test_size=0.2, random_state=42\n","    )\n","\n","    train_dataset = SentimentDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n","    test_dataset = SentimentDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=64)\n","\n","    vocab_size = len(vectorizer.vocabulary_)\n","    embed_dim = 100\n","    hidden_dim = 128\n","    output_dim = 3\n","\n","    print(\"Initializing model...\")\n","    model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim).to(DEVICE)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    print(f\"Using device: {DEVICE}\")\n","\n","    print(\"Training model...\")\n","    for epoch in range(7):\n","        train_loss = train_model(model, train_loader, optimizer, criterion)\n","        val_loss, val_accuracy, _, _ = evaluate_model(model, test_loader, criterion)\n","        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n","\n","    print(\"Saving model...\")\n","    os.makedirs(\"models\", exist_ok=True)\n","    torch.save(model.state_dict(), model_save_path)\n","    print(f\"Model saved to {model_save_path}\")\n","\n","    print(\"Evaluating model...\")\n","    criterion = nn.CrossEntropyLoss()\n","    _, test_accuracy, true_labels, predictions = evaluate_model(model, test_loader, criterion)\n","\n","    unique_classes = sorted(set(true_labels))\n","    num_classes = len(unique_classes)\n","\n","    # Generate target names based on the number of classes\n","    default_target_names = [\"Class \" + str(i) for i in range(num_classes)]\n","    custom_target_names = [\"Negative\", \"Neutral\", \"Positive\"][:num_classes]  # Adjust to match detected classes\n","\n","    print(\"\\nTest Accuracy:\", test_accuracy)\n","    print(\"\\nAccuracy:\", accuracy_score(true_labels, predictions))\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(true_labels, predictions, target_names=custom_target_names))\n","\n","    print(\"\\nConfusion Matrix:\")\n","    print(confusion_matrix(true_labels, predictions))"]},{"cell_type":"markdown","source":["# Final LSTM Model and Evaluation\n","` used in the final version of the project`"],"metadata":{"id":"p3khmdI-Cjl5"}},{"cell_type":"code","source":["import os\n","import pickle\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import pandas as pd\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n","\n","seed = 42\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","if torch.backends.cudnn.is_available():\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=100):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokenizer(self.texts[idx])\n","        tokens = tokens[:self.max_length]  # Truncate if too long\n","        tokens = [0] * (self.max_length - len(tokens)) + tokens  # Pad if too short\n","\n","        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n","        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return tokens_tensor, label_tensor\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=2, dropout=0.5):\n","        super(LSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        hidden_state = lstm_out[:, -1, :]\n","        output = self.fc(hidden_state)\n","        return output\n","\n","def train_model(model, dataloader, optimizer, criterion):\n","    model.train()\n","    total_loss = 0.0\n","    for inputs, labels in dataloader:\n","        inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    true_labels, predictions = [], []\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","            predictions.extend(preds)\n","            true_labels.extend(labels.cpu().numpy())\n","    #return true_labels, predictions\n","    accuracy = accuracy_score(true_labels, predictions)\n","    return total_loss / len(dataloader), accuracy, true_labels, predictions\n","\n","if __name__ == \"__main__\":\n","    vectorizer_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/vectorizer.pkl\"\n","    processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","    model_save_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/lstm_model.pt\"\n","    metric_save_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/results/metrics/\"\n","\n","    print(\"Loading vectorizer...\")\n","    with open(vectorizer_path, \"rb\") as f:\n","        vectorizer = pickle.load(f)\n","\n","    print(\"Loading preprocessed data...\")\n","    data = pd.read_csv(processed_data_path)\n","    data.dropna(subset=[\"text\"], inplace=True)\n","\n","    tokenizer_fn = vectorizer.build_analyzer()\n","\n","    def tokenizer(text):\n","        tokens = tokenizer_fn(text)\n","        return [vectorizer.vocabulary_.get(token, 0) for token in tokens]\n","\n","    print(\"Splitting data into train and test sets...\")\n","    train_texts, test_texts, train_labels, test_labels = train_test_split(\n","        data[\"text\"], data[\"sentiment\"], test_size=0.2, random_state=42\n","    )\n","\n","    train_dataset = SentimentDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n","    test_dataset = SentimentDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=64)\n","\n","    vocab_size = len(vectorizer.vocabulary_)\n","    embed_dim = 100\n","    hidden_dim = 256\n","    output_dim = 3\n","\n","    print(\"Initializing model...\")\n","    model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim).to(DEVICE)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    print(f\"Using device: {DEVICE}\")\n","\n","    print(\"Training model...\")\n","\n","    train_losses = []\n","    val_losses = []\n","    val_accuracies = []\n","\n","    for epoch in range(10):\n","        train_loss = train_model(model, train_loader, optimizer, criterion)\n","        val_loss, val_accuracy, _, _ = evaluate_model(model, test_loader, criterion)\n","\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","        val_accuracies.append(val_accuracy)\n","\n","        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}, Val Accuracy = {val_accuracy:.4f}\")\n","\n","    print(\"Evaluating model...\")\n","    _, test_accuracy, true_labels, predictions = evaluate_model(model, test_loader, criterion)\n","\n","    print(\"Saving model and metrics...\")\n","    #os.makedirs(\"models\", exist_ok=True)\n","    os.makedirs(metric_save_path, exist_ok=True)\n","    torch.save(model.state_dict(), model_save_path)\n","    print(f\"Model saved to {model_save_path}\")\n","    torch.save({'train_losses': train_losses,'val_losses': val_losses,'val_accuracies': val_accuracies}, metric_save_path + \"lstm_training_history2.pth\")\n","    torch.save(true_labels, metric_save_path + \"lstm_true_labels2.pth\")\n","    torch.save(predictions, metric_save_path + \"lstm_predictions2.pth\")\n","    print(f\"Metrics saved to {metric_save_path}\")\n","\n","    unique_classes = sorted(set(true_labels))\n","    num_classes = len(unique_classes)\n","\n","    # Generate target names based on the number of classes\n","    default_target_names = [\"Class \" + str(i) for i in range(num_classes)]\n","    custom_target_names = [\"Negative\", \"Neutral\", \"Positive\"][:num_classes]  # Adjust to match detected classes\n","\n","    print(\"\\nAccuracy:\", accuracy_score(true_labels, predictions))\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(true_labels, predictions, target_names=custom_target_names))\n","\n","    print(\"\\nConfusion Matrix:\")\n","    print(confusion_matrix(true_labels, predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CnQlJ5vM_htn","executionInfo":{"status":"ok","timestamp":1737939613501,"user_tz":-60,"elapsed":138472,"user":{"displayName":"Buğra","userId":"13429055383207080256"}},"outputId":"c284ec90-8dbc-4b29-81ab-70fce821514c"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Loading vectorizer...\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Loading preprocessed data...\n","Splitting data into train and test sets...\n","Initializing model...\n","Using device: cuda\n","Training model...\n","Epoch 1: Train Loss = 0.4250, Val Loss = 0.3979, Val Accuracy = 0.8182\n","Epoch 2: Train Loss = 0.3800, Val Loss = 0.3914, Val Accuracy = 0.8222\n","Epoch 3: Train Loss = 0.3584, Val Loss = 0.3921, Val Accuracy = 0.8228\n","Epoch 4: Train Loss = 0.3394, Val Loss = 0.3941, Val Accuracy = 0.8226\n","Epoch 5: Train Loss = 0.3228, Val Loss = 0.4022, Val Accuracy = 0.8202\n","Epoch 6: Train Loss = 0.3088, Val Loss = 0.4150, Val Accuracy = 0.8190\n","Epoch 7: Train Loss = 0.2986, Val Loss = 0.4164, Val Accuracy = 0.8173\n","Epoch 8: Train Loss = 0.2904, Val Loss = 0.4266, Val Accuracy = 0.8158\n","Epoch 9: Train Loss = 0.2846, Val Loss = 0.4348, Val Accuracy = 0.8151\n","Epoch 10: Train Loss = 0.2809, Val Loss = 0.4341, Val Accuracy = 0.8125\n","Evaluating model...\n","Saving model and metrics...\n","Model saved to /content/drive/MyDrive/Colab Notebooks/SentimentAnalysis/wout_metadata/lstm_model_last.pt\n","Metrics saved to /content/drive/MyDrive/Colab Notebooks/SentimentAnalysis/wout_metadata/results/metrics/\n","\n","Accuracy: 0.8124847353819755\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.81      0.81      0.81    159215\n","     Neutral       0.81      0.81      0.81    160151\n","\n","    accuracy                           0.81    319366\n","   macro avg       0.81      0.81      0.81    319366\n","weighted avg       0.81      0.81      0.81    319366\n","\n","\n","Confusion Matrix:\n","[[129104  30111]\n"," [ 29775 130376]]\n"]}]},{"cell_type":"code","source":["import os\n","import pickle\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n","import pandas as pd\n","\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Set random seed for reproducibility\n","seed = 42\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","if torch.backends.cudnn.is_available():\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","\n","class SentimentDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_length=100):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        tokens = self.tokenizer(self.texts[idx])\n","        tokens = tokens[:self.max_length]\n","        tokens = [0] * (self.max_length - len(tokens)) + tokens\n","\n","        tokens_tensor = torch.tensor(tokens, dtype=torch.long)\n","        label_tensor = torch.tensor(self.labels[idx], dtype=torch.long)\n","\n","        return tokens_tensor, label_tensor\n","\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, n_layers=2, dropout=0.5):\n","        super(LSTMClassifier, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_dim)\n","        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","\n","    def forward(self, x):\n","        embedded = self.embedding(x)\n","        lstm_out, _ = self.lstm(embedded)\n","        hidden_state = lstm_out[:, -1, :]\n","        output = self.fc(hidden_state)\n","        return output\n","\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0.0\n","    true_labels, predictions = [], []\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_loss += loss.item()\n","            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n","            predictions.extend(preds)\n","            true_labels.extend(labels.cpu().numpy())\n","    accuracy = accuracy_score(true_labels, predictions)\n","    return total_loss / len(dataloader), accuracy, true_labels, predictions\n","\n","\n","if __name__ == \"__main__\":\n","    vectorizer_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/vectorizer.pkl\"\n","    processed_data_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/preprocessed.csv\"\n","    model_save_path = \"/content/drive/MyDrive/Akademik/btu/SentimentAnalysis/Colab/models/lstm_model.pt\"\n","\n","    print(\"Loading vectorizer...\")\n","    with open(vectorizer_path, \"rb\") as f:\n","        vectorizer = pickle.load(f)\n","\n","    print(\"Loading preprocessed data...\")\n","    data = pd.read_csv(processed_data_path)\n","    data.dropna(subset=[\"text\"], inplace=True)\n","\n","    tokenizer_fn = vectorizer.build_analyzer()\n","\n","    def tokenizer(text):\n","        tokens = tokenizer_fn(text)\n","        return [vectorizer.vocabulary_.get(token, 0) for token in tokens]\n","\n","    print(\"Preparing test dataset...\")\n","    test_texts, test_labels = data[\"text\"].tolist(), data[\"sentiment\"].tolist()\n","    test_dataset = SentimentDataset(test_texts, test_labels, tokenizer)\n","    test_loader = DataLoader(test_dataset, batch_size=64)\n","\n","    vocab_size = len(vectorizer.vocabulary_)\n","    embed_dim = 100\n","    hidden_dim = 256\n","    output_dim = 3\n","\n","    print(\"Loading model...\")\n","    model = LSTMClassifier(vocab_size, embed_dim, hidden_dim, output_dim).to(DEVICE)\n","    model.load_state_dict(torch.load(model_save_path))\n","\n","    print(\"Evaluating model...\")\n","    criterion = nn.CrossEntropyLoss()\n","    _, test_accuracy, true_labels, predictions = evaluate_model(model, test_loader, criterion)\n","\n","    unique_classes = sorted(set(true_labels))\n","    num_classes = len(unique_classes)\n","\n","    # Generate target names based on the number of classes\n","    default_target_names = [\"Class \" + str(i) for i in range(num_classes)]\n","    custom_target_names = [\"Negative\", \"Neutral\", \"Positive\"][:num_classes]  # Adjust to match detected classes\n","\n","    print(\"\\nAccuracy:\", accuracy_score(true_labels, predictions))\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(true_labels, predictions, target_names=custom_target_names))\n","\n","    print(\"\\nConfusion Matrix:\")\n","    print(confusion_matrix(true_labels, predictions, labels=unique_classes))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9RuZPr2rR5aL","executionInfo":{"status":"ok","timestamp":1737939983658,"user_tz":-60,"elapsed":214958,"user":{"displayName":"Buğra","userId":"13429055383207080256"}},"outputId":"c901b3fd-3cd1-4d80-b270-5fac61845235"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading vectorizer...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.3.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.3.2 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Loading preprocessed data...\n","Preparing test dataset...\n","Loading model...\n","Evaluating model...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-c5a77871de0e>:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load(model_save_path))\n"]},{"output_type":"stream","name":"stdout","text":["\n","Accuracy: 0.8780677968224545\n","\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","    Negative       0.88      0.88      0.88    798396\n","     Neutral       0.88      0.88      0.88    798434\n","\n","    accuracy                           0.88   1596830\n","   macro avg       0.88      0.88      0.88   1596830\n","weighted avg       0.88      0.88      0.88   1596830\n","\n","\n","Confusion Matrix:\n","[[699583  98813]\n"," [ 95892 702542]]\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true,"mount_file_id":"1hGrbx5wsqm-CL9LU36tCeHjvQo7PNWZP","authorship_tag":"ABX9TyMnNtPQ0KQqokBQLSfl60YZ"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"96b72f6e7fe34912a8936e5fdb3c08ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4017635b677f40d3a66330c0e7460603","IPY_MODEL_79add1aa667441fd9441671335003411","IPY_MODEL_b71e8c590c5045e0859c2e5b498138d9"],"layout":"IPY_MODEL_27a1723962df444db8c9c020ea30e49e"}},"4017635b677f40d3a66330c0e7460603":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69e326f98da44bec84d049c2c61bd0d0","placeholder":"​","style":"IPY_MODEL_f867c03820cd4f12995a822b739275a9","value":"tokenizer_config.json: 100%"}},"79add1aa667441fd9441671335003411":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_21f625ec2e1644d4ad6f351acd0ffd28","max":48,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a098da9765704755b8bf22c859f84829","value":48}},"b71e8c590c5045e0859c2e5b498138d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0d25bb222bf4b5498ebccb1aacd2598","placeholder":"​","style":"IPY_MODEL_86b4b1ca87264494b2327cf2c639c80f","value":" 48.0/48.0 [00:00&lt;00:00, 3.03kB/s]"}},"27a1723962df444db8c9c020ea30e49e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69e326f98da44bec84d049c2c61bd0d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f867c03820cd4f12995a822b739275a9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"21f625ec2e1644d4ad6f351acd0ffd28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a098da9765704755b8bf22c859f84829":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c0d25bb222bf4b5498ebccb1aacd2598":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86b4b1ca87264494b2327cf2c639c80f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c3abfe77876a4ce0a65b1221f72f413a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_94917daf0b8c4b89b1627ab755c72472","IPY_MODEL_5827887db6464a609ab61b2b9e3bd383","IPY_MODEL_45f71293c66e47479e9ee589ea247450"],"layout":"IPY_MODEL_003be98a6bb2480a8ed54ee59a46832b"}},"94917daf0b8c4b89b1627ab755c72472":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf32d0766a7b4deeaffbfb3c1f572d2b","placeholder":"​","style":"IPY_MODEL_51888b94585a46b8873aa684710a85ba","value":"vocab.txt: 100%"}},"5827887db6464a609ab61b2b9e3bd383":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_34de4187eb7f49f58fdb3de21d1261a2","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_312cd83d7aaa409dbf9d117a0f59f640","value":231508}},"45f71293c66e47479e9ee589ea247450":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5359ba9e6634305bf32d564b9b16fc5","placeholder":"​","style":"IPY_MODEL_d33a3c8cba894bafa61f41fb2d45bbe3","value":" 232k/232k [00:00&lt;00:00, 5.17MB/s]"}},"003be98a6bb2480a8ed54ee59a46832b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf32d0766a7b4deeaffbfb3c1f572d2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51888b94585a46b8873aa684710a85ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"34de4187eb7f49f58fdb3de21d1261a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"312cd83d7aaa409dbf9d117a0f59f640":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e5359ba9e6634305bf32d564b9b16fc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d33a3c8cba894bafa61f41fb2d45bbe3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09dd094ecdc2418193bcd4fd4d5b5c26":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3da5d9c42cd049b69e48e29f9194a0d9","IPY_MODEL_14cd7cb4ad254b5492903648353c2940","IPY_MODEL_83bd0e9a829140cfa9b1dbdfab2b2965"],"layout":"IPY_MODEL_9c4fb0f182034c9da97cdb4b3cee15dc"}},"3da5d9c42cd049b69e48e29f9194a0d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b335529231b443cb8f662b8ca11197d6","placeholder":"​","style":"IPY_MODEL_0d156bb74e4841d1825acff2fc298553","value":"tokenizer.json: 100%"}},"14cd7cb4ad254b5492903648353c2940":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_de721d7624aa45de9db9dc8472fc4ee8","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ddaf99001c3b4d2c8b0e0c6a2293eb36","value":466062}},"83bd0e9a829140cfa9b1dbdfab2b2965":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d0a7db42d6f74a9fb40557317d951716","placeholder":"​","style":"IPY_MODEL_b9e1c557a2d343a8a05ec7b361872721","value":" 466k/466k [00:00&lt;00:00, 10.3MB/s]"}},"9c4fb0f182034c9da97cdb4b3cee15dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b335529231b443cb8f662b8ca11197d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d156bb74e4841d1825acff2fc298553":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de721d7624aa45de9db9dc8472fc4ee8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddaf99001c3b4d2c8b0e0c6a2293eb36":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d0a7db42d6f74a9fb40557317d951716":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9e1c557a2d343a8a05ec7b361872721":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc7f443540534bebb46342243e685dd6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_68cf654b4c3f42578ebd6a28f403b87c","IPY_MODEL_7f5a66af9d2a46668eee21b22e280758","IPY_MODEL_9b3639fb05a64fa79315bcc77f5a2cd8"],"layout":"IPY_MODEL_fed8747baa464bb1a73a5e77048889fa"}},"68cf654b4c3f42578ebd6a28f403b87c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a97e13f873e4abf9e56147bc2298914","placeholder":"​","style":"IPY_MODEL_be30bfcfff314c42aee3fd613a7e6f2a","value":"config.json: 100%"}},"7f5a66af9d2a46668eee21b22e280758":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_240f089a45404b939264b670e88c2b99","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d00ffd748f994258a79628dd2ece8d1b","value":570}},"9b3639fb05a64fa79315bcc77f5a2cd8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a6df3ece7a44ebf9bd1e83328909e18","placeholder":"​","style":"IPY_MODEL_ea783fb21f46478ea508a9088a26fabf","value":" 570/570 [00:00&lt;00:00, 44.7kB/s]"}},"fed8747baa464bb1a73a5e77048889fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a97e13f873e4abf9e56147bc2298914":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be30bfcfff314c42aee3fd613a7e6f2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"240f089a45404b939264b670e88c2b99":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d00ffd748f994258a79628dd2ece8d1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6a6df3ece7a44ebf9bd1e83328909e18":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea783fb21f46478ea508a9088a26fabf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3e8af17130d544a38a65a16094a557a7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a0b326ee2d3744298683a660cc3dd6b7","IPY_MODEL_6787782c7ef349859f08f3d527cbe1fd","IPY_MODEL_89b745e2c3d340c391a85174ce6e9180"],"layout":"IPY_MODEL_bd6b2443cbde43a186b68c020db89720"}},"a0b326ee2d3744298683a660cc3dd6b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_34f7d207a3dd4048976a9cc91f50a007","placeholder":"​","style":"IPY_MODEL_92f10d814f4e440a9c926f302e339fbf","value":"model.safetensors: 100%"}},"6787782c7ef349859f08f3d527cbe1fd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7994faa703814f2ba8413e6619a97ce0","max":440449768,"min":0,"orientation":"horizontal","style":"IPY_MODEL_79adbb7b670d4c0e9e58021608d7cf9f","value":440449768}},"89b745e2c3d340c391a85174ce6e9180":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e9cd3d7d3b33465c9456ba9dd408ab65","placeholder":"​","style":"IPY_MODEL_2a183c6550de477d9b4b65151d65a2f9","value":" 440M/440M [00:02&lt;00:00, 230MB/s]"}},"bd6b2443cbde43a186b68c020db89720":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34f7d207a3dd4048976a9cc91f50a007":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92f10d814f4e440a9c926f302e339fbf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7994faa703814f2ba8413e6619a97ce0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"79adbb7b670d4c0e9e58021608d7cf9f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e9cd3d7d3b33465c9456ba9dd408ab65":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a183c6550de477d9b4b65151d65a2f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}